{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1624,
     "status": "ok",
     "timestamp": 1730229867747,
     "user": {
      "displayName": "Nishal Chandra Reddy",
      "userId": "06667136568360495769"
     },
     "user_tz": 420
    },
    "id": "ktzL_yM7GCYB",
    "outputId": "61fa4988-7584-4546-db08-00a868001c1f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1730229867747,
     "user": {
      "displayName": "Nishal Chandra Reddy",
      "userId": "06667136568360495769"
     },
     "user_tz": 420
    },
    "id": "CQfpXXI6L_NB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, dataframe, window_length=100):\n",
    "        # Perform the custom transformation\n",
    "        sliced_df = self.custom_transformation(dataframe.to_numpy(), window_length=window_length)\n",
    "        self.data = torch.tensor(sliced_df, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of trajectories\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the trajectory at the given index\n",
    "        return self.data[idx]\n",
    "\n",
    "    def custom_transformation(self, dataframe_array, window_length):\n",
    "        window_length += 1  # get one more column as targets\n",
    "\n",
    "        # Preallocate memory for the slices\n",
    "        sliced_data = np.lib.stride_tricks.sliding_window_view(dataframe_array, window_shape=(window_length,), axis=1)\n",
    "\n",
    "        # Reshape into a flat 2D array for DataFrame-like output\n",
    "        sliced_data = sliced_data.reshape(-1, window_length)\n",
    "\n",
    "        return sliced_data\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = nn.Linear(hidden_size * 2, 1, bias=False)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attn_weights = torch.softmax(self.attention(lstm_output), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_output, dim=1)\n",
    "        return context\n",
    "\n",
    "\n",
    "# Define the BiLSTM with Attention model\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # *2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_output, _ = self.lstm(x, (h0, c0))\n",
    "        context = self.attention(lstm_output)\n",
    "        out = self.fc(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbLEt3JJL_NG"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8083,
     "status": "ok",
     "timestamp": 1730229866140,
     "user": {
      "displayName": "Nishal Chandra Reddy",
      "userId": "06667136568360495769"
     },
     "user_tz": 420
    },
    "id": "3NX2jH76L_NM",
    "outputId": "4e72d38d-2c85-440a-80bd-bceab46dcd5a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = \"./\"\n",
    "# dataset_path = \"/content/drive/Othercomputers/My Laptop/Sem_3/CSE_575_SML/Projects/Individual_Project/dataset/\"\n",
    "\n",
    "# Get the relative path of a file in the current working directory\n",
    "train_path = os.path.join(dataset_path + \"train.csv\")\n",
    "val_path = os.path.join(dataset_path + \"val.csv\")\n",
    "test_path = os.path.join(dataset_path + \"test.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_path, header=0).drop(\"ids\", axis=1)\n",
    "val_df = pd.read_csv(val_path, header=0).drop(\"ids\", axis=1)\n",
    "test_df = pd.read_csv(test_path, header=0).drop(\"ids\", axis=1)\n",
    "\n",
    "# print the training data shape\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Validation data shape: {val_df.shape}\")\n",
    "print(f\"Testing data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1730229866142,
     "user": {
      "displayName": "Nishal Chandra Reddy",
      "userId": "06667136568360495769"
     },
     "user_tz": 420
    },
    "id": "rnAfnsS8rTta"
   },
   "outputs": [],
   "source": [
    "# Check if MPS is available and set the device accordingly\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "# Hyperparameters\n",
    "window_length = 100\n",
    "batch_size = 64\n",
    "input_size = 1  # For univariate time series\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "output_size = 1  # For univariate time series prediction\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSkr7fD4rcmU"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader for training\n",
    "dataset = TrajectoryDataset(dataframe=train_df, window_length=window_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the model, loss function, optimizer, and scheduler\n",
    "model = BiLSTMWithAttention(input_size, hidden_size, num_layers, output_size, dropout=0.5).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 377175,
     "status": "error",
     "timestamp": 1730229353168,
     "user": {
      "displayName": "Nishal Chandra Reddy",
      "userId": "06667136568360495769"
     },
     "user_tz": 420
    },
    "id": "0a2wcGz-L_NN",
    "outputId": "33955059-771e-4458-c72f-01d96c03b3fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to save the model checkpoints\n",
    "checkpoint_path = \"model_checkpoint.pth\"\n",
    "\n",
    "# Function to save the model checkpoint\n",
    "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, path)\n",
    "\n",
    "# Function to load the model checkpoint\n",
    "def load_checkpoint(model, optimizer, path):\n",
    "    if os.path.isfile(path):\n",
    "        checkpoint = torch.load(path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f\"Checkpoint loaded: epoch {epoch}, loss {loss:.4f}\")\n",
    "        return epoch, loss\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, float('inf')\n",
    "\n",
    "# Load the checkpoint if it exists\n",
    "start_epoch, _ = load_checkpoint(model, optimizer, checkpoint_path)\n",
    "\n",
    "# Training loop with tqdm for progress tracking\n",
    "for epoch in tqdm(range(start_epoch, num_epochs), desc=\"Epochs\", unit=\"epoch\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Use tqdm to track batch progress within each epoch\n",
    "    for batch_idx, data in tqdm(enumerate(dataloader), desc=f\"Epoch {epoch + 1}\", unit=\"batch\", leave=False):\n",
    "\n",
    "        # Separate inputs and targets\n",
    "        inputs = data[:, :-1].unsqueeze(2).to(device)\n",
    "        targets = data[:, -1].to(device)  # Last column is the target (next value)\n",
    "\n",
    "        optimizer.zero_grad() # Zero the parameter gradients\n",
    "\n",
    "        outputs = model(inputs) # Forward pass\n",
    "\n",
    "        loss = criterion(outputs.squeeze(), targets) # Compute the loss\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    # Save the checkpoint at the end of each epoch\n",
    "    save_checkpoint(model, optimizer, epoch + 1, running_loss / len(dataloader), checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nbSzpKLL_NR"
   },
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nmjzvvnL_NU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Autoregressive prediction function\n",
    "def autoregressive_predict(model, input_matrix, prediction_length):\n",
    "    \"\"\"\n",
    "    Perform autoregressive prediction using the learned model.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained PyTorch model.\n",
    "    - input_matrix: A matrix of initial time steps (e.g., shape (963, window_length)).\n",
    "    - prediction_length: The length of the future trajectory to predict.\n",
    "\n",
    "    Returns:\n",
    "    - output_matrix: A tensor of the predicted future trajectory of the same length as `prediction_length`.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    input_matrix = input_matrix.to(next(model.parameters()).device)  # Move to model's device\n",
    "    output_matrix = torch.empty(input_matrix.shape[0], 0).to(next(model.parameters()).device)  # Initialize on the model's device\n",
    "    current_input = input_matrix\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients for prediction\n",
    "        for _ in range(prediction_length):\n",
    "            # Predict the next time step\n",
    "            next_pred = model(current_input.unsqueeze(2))\n",
    "\n",
    "            # Concatenating the new column along dimension 1 (columns)\n",
    "            output_matrix = torch.cat((output_matrix, next_pred), dim=1)\n",
    "\n",
    "            # Use the predicted value as part of the next input\n",
    "            current_input = torch.cat((current_input[:, 1:], next_pred), dim=1)\n",
    "\n",
    "    return output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot0-tAqdEKxv"
   },
   "outputs": [],
   "source": [
    "# Prepare validation and test data\n",
    "train_set = torch.tensor(train_df.values[:, :].astype(np.float32), dtype=torch.float32).to(device)  # Move to the appropriate device\n",
    "val_set = torch.tensor(val_df.values[:, :].astype(np.float32), dtype=torch.float32).to(device)  # Move to the appropriate device\n",
    "test_set = torch.tensor(val_df.values[:, :].astype(np.float32), dtype=torch.float32).to(device)  # Move to the appropriate device\n",
    "\n",
    "\n",
    "# Generate predictions for validation set\n",
    "initial_input = train_set[:, -window_length:]  # use the last window of training set as initial input\n",
    "val_predictions_tensor = autoregressive_predict(model, initial_input, val_set.shape[1])\n",
    "print(f\"Validation Predictions Tensor Shape: {val_predictions_tensor.shape}\")\n",
    "\n",
    "# Calculate MSE between predicted trajectories and actual validation trajectories for validation\n",
    "mse_loss = nn.MSELoss()  # Calculate MSE for validation set\n",
    "mse = mse_loss(val_predictions_tensor, val_set)  # Compute MSE\n",
    "print(f\"Autoregressive Validation MSE: {mse.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHdAer84L_NX"
   },
   "source": [
    "## Plot it out to see what is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "for row_idx in range(3):\n",
    "    initial_input = val_set[row_idx, :window_length].unsqueeze(0)\n",
    "\n",
    "    # Use the previously generated prediction for the validation set\n",
    "    predicted_trajectory = val_predictions_tensor[row_idx].cpu().numpy()\n",
    "\n",
    "    # Get the actual trajectory for comparison\n",
    "    actual_trajectory = val_set[row_idx].cpu().numpy()\n",
    "\n",
    "    # Plot the actual vs predicted trajectory\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.plot(range(len(actual_trajectory)), actual_trajectory, label=\"Actual Trajectory\", color=\"blue\", marker=\"o\")\n",
    "    plt.plot(range(len(predicted_trajectory)), predicted_trajectory, label=\"Predicted Trajectory\", color=\"red\", linestyle=\"--\", marker=\"x\")\n",
    "    plt.title(f\"Actual vs Predicted Trajectory (Row {row_idx})\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"trajectory_{row_idx}.png\", dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rB41oULAL_NZ"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the test dataset\n",
    "initial_input = val_predictions_tensor[:, -window_length:]\n",
    "test_predictions_tensor = autoregressive_predict(model, initial_input, test_set.shape[1])\n",
    "\n",
    "print(f\"Test Predictions Tensor Shape: {test_predictions_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_a_f55PL_Nb"
   },
   "outputs": [],
   "source": [
    "def generate_submissions_v4(pred_val_tensor, pred_test_tensor, original_val_path, original_test_path):\n",
    "    # Read the original validation and testing datasets\n",
    "    original_val_df = pd.read_csv(original_val_path)\n",
    "    original_test_df = pd.read_csv(original_test_path)\n",
    "\n",
    "    # Ensure the shape of pred_val_tensor and pred_test_tensor is correct\n",
    "    assert pred_val_tensor.shape[0] * pred_val_tensor.shape[1] == original_val_df.shape[0] * (original_val_df.shape[1] - 1)\n",
    "    assert pred_test_tensor.shape[0] * pred_test_tensor.shape[1] == original_test_df.shape[0] * (original_test_df.shape[1] - 1)\n",
    "\n",
    "    # Create empty lists to store ids and values\n",
    "    ids = []\n",
    "    values = []\n",
    "\n",
    "    # Process validation set\n",
    "    for col_idx, col in enumerate(original_val_df.columns[1:]):  # Skip the 'ids' column\n",
    "        for row_idx, _ in enumerate(original_val_df[col]):\n",
    "            ids.append(str(f\"{col}_traffic_val_{row_idx}\"))\n",
    "            values.append(float(pred_val_tensor[row_idx, col_idx]))\n",
    "\n",
    "    # Process testing set\n",
    "    for col_idx, col in enumerate(original_test_df.columns[1:]):  # Skip the 'ids' column\n",
    "        for row_idx, _ in enumerate(original_test_df[col]):\n",
    "            ids.append(str(f\"{col}_traffic_test_{row_idx}\"))\n",
    "            values.append(float(pred_test_tensor[row_idx, col_idx]))\n",
    "\n",
    "    # Create the submissions dataframe\n",
    "    submissions_df = pd.DataFrame({\"ids\": ids, \"value\": values})\n",
    "\n",
    "    # Impute any null values\n",
    "    submissions_df.fillna(100, inplace=True)\n",
    "\n",
    "    # Assert the shape of the dataframe\n",
    "    assert submissions_df.shape[1] == 2\n",
    "    assert submissions_df.shape[0] == (original_val_df.shape[0] * (original_val_df.shape[1] - 1)) + (\n",
    "        original_test_df.shape[0] * (original_test_df.shape[1] - 1)\n",
    "    )\n",
    "    assert \"ids\" in submissions_df.columns\n",
    "    assert \"value\" in submissions_df.columns\n",
    "\n",
    "    # Save to CSV\n",
    "    submissions_df.to_csv(\"submissions_v3.csv\", index=False)\n",
    "\n",
    "\n",
    "# Call the function\n",
    "generate_submissions_v4(val_predictions_tensor, test_predictions_tensor, val_path, test_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9826512,
     "sourceId": 84270,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "py_sml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
